{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753ae5b1-2f57-4f1c-ba12-0c2a6f62a23a",
   "metadata": {},
   "source": [
    "# 计算测试集图像语义特征\n",
    "\n",
    "抽取Pytorch训练得到的图像分类模型中间层的输出特征，作为输入图像的语义特征。\n",
    "\n",
    "计算测试集所有图像的语义特征，使用t-SNE和UMAP两种降维方法降维至二维和三维，可视化。\n",
    "\n",
    "分析不同类别的语义距离、异常数据、细粒度分类、高维数据结构。\n",
    "\n",
    "同济子豪兄：https://space.bilibili.com/1900783\n",
    "\n",
    "[代码运行云GPU环境](https://featurize.cn/?s=d7ce99f842414bfcaea5662a97581bd1)：GPU RTX 3060、CUDA v11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a232f-0178-4661-98d8-f27f6fa130bf",
   "metadata": {},
   "source": [
    "## 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57328106-567d-4be0-a0bc-1a20e88ceb01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:13:37.413301900Z",
     "start_time": "2024-09-28T09:13:31.896696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# 忽略烦人的红色提示\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 有 GPU 就用 GPU，没有就用 CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500ea23-53d8-4421-92f6-647d41e34d80",
   "metadata": {},
   "source": [
    "## 图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b562b97f-9a4a-4729-b238-4acb2e77e9dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:13:49.619935800Z",
     "start_time": "2024-09-28T09:13:46.215967100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# # 训练集图像预处理：缩放裁剪、图像增强、转 Tensor、归一化\n",
    "# train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#                                      ])\n",
    "\n",
    "# 测试集图像预处理-RCTN：缩放、裁剪、转 Tensor、归一化\n",
    "test_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(\n",
    "                                         mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3347a-1c09-4b48-b19a-fbcc3791c08b",
   "metadata": {},
   "source": [
    "## 导入训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618333ec-6e91-4342-acf3-86aaa1654068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:41:28.956868600Z",
     "start_time": "2024-09-28T09:41:26.424994900Z"
    }
   },
   "outputs": [],
   "source": [
    "from scripts.grad_cam.swin_model import swin_base_patch4_window7_224\n",
    "model = swin_base_patch4_window7_224(pretrained=False)\n",
    "weights_path = \"../best_model.pth\"\n",
    "model.load_state_dict(torch.load(weights_path, map_location=\"cuda\"), strict=False)\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c4cb2-e516-4150-9e9c-9580a26c72a3",
   "metadata": {},
   "source": [
    "## 抽取模型中间层输出结果作为语义特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9fdf127-c713-4bb4-bcc8-4f808e011229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:38:07.804675800Z",
     "start_time": "2024-09-28T09:38:07.781659200Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4dda05-7510-40a5-bc6f-6df80fdc4c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:38:09.413682600Z",
     "start_time": "2024-09-28T09:38:09.274360Z"
    }
   },
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTraceError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model_trunc \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_feature_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mavgpool\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msemantic_feature\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torchvision\\models\\feature_extraction.py:485\u001B[0m, in \u001B[0;36mcreate_feature_extractor\u001B[1;34m(model, return_nodes, train_return_nodes, eval_return_nodes, tracer_kwargs, suppress_diff_warning)\u001B[0m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# Instantiate our NodePathTracer and use that to trace the model\u001B[39;00m\n\u001B[0;32m    484\u001B[0m tracer \u001B[38;5;241m=\u001B[39m NodePathTracer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtracer_kwargs)\n\u001B[1;32m--> 485\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    487\u001B[0m name \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, nn\u001B[38;5;241m.\u001B[39mModule) \u001B[38;5;28;01melse\u001B[39;00m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m    488\u001B[0m graph_module \u001B[38;5;241m=\u001B[39m fx\u001B[38;5;241m.\u001B[39mGraphModule(tracer\u001B[38;5;241m.\u001B[39mroot, graph, name)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\fx\\_symbolic_trace.py:793\u001B[0m, in \u001B[0;36mTracer.trace\u001B[1;34m(self, root, concrete_args)\u001B[0m\n\u001B[0;32m    786\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_autowrap_search:\n\u001B[0;32m    787\u001B[0m             _autowrap_check(\n\u001B[0;32m    788\u001B[0m                 patcher, module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_autowrap_function_ids\n\u001B[0;32m    789\u001B[0m             )\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_node(\n\u001B[0;32m    791\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    792\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m--> 793\u001B[0m             (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_arg(\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m),),\n\u001B[0;32m    794\u001B[0m             {},\n\u001B[0;32m    795\u001B[0m             type_expr\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__annotations__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    796\u001B[0m         )\n\u001B[0;32m    798\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubmodule_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    799\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Study\\myCode\\py_code\\AI\\FSL-master\\scripts\\grad_cam\\swin_model.py:554\u001B[0m, in \u001B[0;36mSwinTransformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;66;03m# x: [B, L, C]\u001B[39;00m\n\u001B[1;32m--> 554\u001B[0m     x, H, W \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    555\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_drop(x)\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\fx\\_symbolic_trace.py:771\u001B[0m, in \u001B[0;36mTracer.trace.<locals>.module_call_wrapper\u001B[1;34m(mod, *args, **kwargs)\u001B[0m\n\u001B[0;32m    764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _orig_module_call(mod, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    766\u001B[0m _autowrap_check(\n\u001B[0;32m    767\u001B[0m     patcher,\n\u001B[0;32m    768\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(mod, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforward\u001B[39m\u001B[38;5;124m\"\u001B[39m, mod), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__globals__\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}),\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_autowrap_function_ids,\n\u001B[0;32m    770\u001B[0m )\n\u001B[1;32m--> 771\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torchvision\\models\\feature_extraction.py:83\u001B[0m, in \u001B[0;36mNodePathTracer.call_module\u001B[1;34m(self, m, forward, args, kwargs)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_module_qualname \u001B[38;5;241m=\u001B[39m module_qualname\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_leaf_module(m, module_qualname):\n\u001B[1;32m---> 83\u001B[0m     out \u001B[38;5;241m=\u001B[39m forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_proxy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcall_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, module_qualname, args, kwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\fx\\_symbolic_trace.py:764\u001B[0m, in \u001B[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    763\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _orig_module_call(mod, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Study\\myCode\\py_code\\AI\\FSL-master\\scripts\\grad_cam\\swin_model.py:105\u001B[0m, in \u001B[0;36mPatchEmbed.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    101\u001B[0m _, _, H, W \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# padding\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;66;03m# 如果输入图片的H，W不是patch_size的整数倍，需要进行padding\u001B[39;00m\n\u001B[1;32m--> 105\u001B[0m pad_input \u001B[38;5;241m=\u001B[39m (H \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (W \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pad_input:\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;66;03m# to pad the last 3 dimensions,\u001B[39;00m\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;66;03m# (W_left, W_right, H_top,H_bottom, C_front, C_back)\u001B[39;00m\n\u001B[0;32m    109\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mpad(x, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m W \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    110\u001B[0m                   \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m H \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    111\u001B[0m                   \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\fx\\proxy.py:443\u001B[0m, in \u001B[0;36mProxy.__bool__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    440\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtracer\u001B[38;5;241m.\u001B[39mcreate_proxy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcall_function\u001B[39m\u001B[38;5;124m'\u001B[39m, assert_fn, (\u001B[38;5;28mself\u001B[39m,), {})\n\u001B[0;32m    441\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_bool\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\fx\\proxy.py:303\u001B[0m, in \u001B[0;36mTracerBase.to_bool\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;129m@compatibility\u001B[39m(is_backward_compatible\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_bool\u001B[39m(\u001B[38;5;28mself\u001B[39m, obj: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mProxy\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    298\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001B[39;00m\n\u001B[0;32m    300\u001B[0m \u001B[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001B[39;00m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001B[39;00m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 303\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TraceError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTraceError\u001B[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "model_trunc = create_feature_extractor(model, return_nodes={'avgpool': 'semantic_feature'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7137df-1c4a-452e-be96-de7ec6a5d85e",
   "metadata": {},
   "source": [
    "## 计算单张图像的语义特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157f9102-fc51-468e-a43e-1a9256dfe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'fruit30_split/val/菠萝/105.jpg'\n",
    "img_pil = Image.open(img_path)\n",
    "input_img = test_transform(img_pil) # 预处理\n",
    "input_img = input_img.unsqueeze(0).to(device)\n",
    "# 执行前向预测，得到指定中间层的输出\n",
    "pred_logits = model_trunc(input_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "108eb111-dc92-408c-8bf8-6111ad6a28e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logits['semantic_feature'].squeeze().detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70ba1f3-02c3-4bc7-92c2-5b0ee5a76762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_logits['semantic_feature'].squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d681eb-d2af-4e54-bfa2-78590dffafbe",
   "metadata": {},
   "source": [
    "## 载入测试集图像分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb76734-83d2-49c9-ae35-3240595d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('测试集预测结果.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de3a2b3-0035-4f01-afbc-69b87e1af806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>图像路径</th>\n",
       "      <th>标注类别ID</th>\n",
       "      <th>标注类别名称</th>\n",
       "      <th>top-1-预测ID</th>\n",
       "      <th>top-1-预测名称</th>\n",
       "      <th>top-2-预测ID</th>\n",
       "      <th>top-2-预测名称</th>\n",
       "      <th>top-3-预测ID</th>\n",
       "      <th>top-3-预测名称</th>\n",
       "      <th>top-n预测正确</th>\n",
       "      <th>...</th>\n",
       "      <th>草莓-预测置信度</th>\n",
       "      <th>荔枝-预测置信度</th>\n",
       "      <th>菠萝-预测置信度</th>\n",
       "      <th>葡萄-白-预测置信度</th>\n",
       "      <th>葡萄-红-预测置信度</th>\n",
       "      <th>西瓜-预测置信度</th>\n",
       "      <th>西红柿-预测置信度</th>\n",
       "      <th>车厘子-预测置信度</th>\n",
       "      <th>香蕉-预测置信度</th>\n",
       "      <th>黄瓜-预测置信度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fruit30_split/val/哈密瓜/106.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>4.0</td>\n",
       "      <td>柚子</td>\n",
       "      <td>5.0</td>\n",
       "      <td>柠檬</td>\n",
       "      <td>7.0</td>\n",
       "      <td>梨</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810175e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>1.294236e-04</td>\n",
       "      <td>3.994173e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5.830796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fruit30_split/val/哈密瓜/109.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>6.0</td>\n",
       "      <td>桂圆</td>\n",
       "      <td>0.0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>8.0</td>\n",
       "      <td>椰子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.460142e-08</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>7.725556e-07</td>\n",
       "      <td>3.171619e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>2.559105e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fruit30_split/val/哈密瓜/114.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>26.0</td>\n",
       "      <td>西红柿</td>\n",
       "      <td>23.0</td>\n",
       "      <td>葡萄-白</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.829248e-03</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.035187</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>2.761092e-01</td>\n",
       "      <td>1.695518e-04</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>1.219466e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fruit30_split/val/哈密瓜/116.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>16.0</td>\n",
       "      <td>芒果</td>\n",
       "      <td>4.0</td>\n",
       "      <td>柚子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.417844e-05</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>8.358340e-04</td>\n",
       "      <td>2.168997e-07</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>4.123446e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fruit30_split/val/哈密瓜/118.png</td>\n",
       "      <td>0</td>\n",
       "      <td>哈密瓜</td>\n",
       "      <td>4.0</td>\n",
       "      <td>柚子</td>\n",
       "      <td>11.0</td>\n",
       "      <td>猕猴桃</td>\n",
       "      <td>23.0</td>\n",
       "      <td>葡萄-白</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.725807e-04</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.091698</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>3.796561e-03</td>\n",
       "      <td>3.087181e-08</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>5.176165e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            图像路径  标注类别ID 标注类别名称  top-1-预测ID top-1-预测名称  \\\n",
       "0  fruit30_split/val/哈密瓜/106.jpg       0    哈密瓜         4.0         柚子   \n",
       "1  fruit30_split/val/哈密瓜/109.jpg       0    哈密瓜         6.0         桂圆   \n",
       "2  fruit30_split/val/哈密瓜/114.jpg       0    哈密瓜         0.0        哈密瓜   \n",
       "3  fruit30_split/val/哈密瓜/116.jpg       0    哈密瓜         0.0        哈密瓜   \n",
       "4  fruit30_split/val/哈密瓜/118.png       0    哈密瓜         4.0         柚子   \n",
       "\n",
       "   top-2-预测ID top-2-预测名称  top-3-预测ID top-3-预测名称  top-n预测正确  ...      草莓-预测置信度  \\\n",
       "0         5.0         柠檬         7.0          梨        0.0  ...  1.810175e-07   \n",
       "1         0.0        哈密瓜         8.0         椰子        1.0  ...  8.460142e-08   \n",
       "2        26.0        西红柿        23.0       葡萄-白        1.0  ...  9.829248e-03   \n",
       "3        16.0         芒果         4.0         柚子        1.0  ...  4.417844e-05   \n",
       "4        11.0        猕猴桃        23.0       葡萄-白        0.0  ...  7.725807e-04   \n",
       "\n",
       "   荔枝-预测置信度  菠萝-预测置信度  葡萄-白-预测置信度  葡萄-红-预测置信度  西瓜-预测置信度     西红柿-预测置信度  \\\n",
       "0  0.000001  0.000003    0.000010    0.000006  0.000111  1.294236e-04   \n",
       "1  0.000001  0.000001    0.001481    0.000045  0.000175  7.725556e-07   \n",
       "2  0.007687  0.001150    0.040230    0.035187  0.001550  2.761092e-01   \n",
       "3  0.000247  0.000071    0.001455    0.000003  0.000460  8.358340e-04   \n",
       "4  0.000075  0.000089    0.091698    0.000659  0.000463  3.796561e-03   \n",
       "\n",
       "      车厘子-预测置信度  香蕉-预测置信度      黄瓜-预测置信度  \n",
       "0  3.994173e-07  0.000004  5.830796e-07  \n",
       "1  3.171619e-06  0.000033  2.559105e-06  \n",
       "2  1.695518e-04  0.006084  1.219466e-03  \n",
       "3  2.168997e-07  0.022086  4.123446e-04  \n",
       "4  3.087181e-08  0.000306  5.176165e-04  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7d010-df35-4fb9-8dcb-1af2d5399681",
   "metadata": {},
   "source": [
    "## 计算测试集每张图像的语义特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d630667-6b6b-4fdd-8c4d-28286499330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:15<00:00, 71.71it/s]\n"
     ]
    }
   ],
   "source": [
    "encoding_array = []\n",
    "img_path_list = []\n",
    "\n",
    "for img_path in tqdm(df['图像路径']):\n",
    "    img_path_list.append(img_path)\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "    input_img = test_transform(img_pil).unsqueeze(0).to(device) # 预处理\n",
    "    feature = model_trunc(input_img)['semantic_feature'].squeeze().detach().cpu().numpy() # 执行前向预测，得到 avgpool 层输出的语义特征\n",
    "    encoding_array.append(feature)\n",
    "encoding_array = np.array(encoding_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec12f5b-35d0-4367-8ac1-7d7321eecee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1079, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434aad3-9689-4e77-bef7-1daa4400e590",
   "metadata": {},
   "source": [
    "## 保存为本地的.npy文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "896a318d-430c-4b59-9c94-0b08ba0106dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为本地的 npy 文件\n",
    "np.save('测试集语义特征.npy', encoding_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417cdc4-0bad-428c-896f-1d35d31401a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
